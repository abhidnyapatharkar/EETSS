{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81dd0a17",
   "metadata": {},
   "source": [
    "##### Dataset Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089b4606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This data set was provided by Fraunhofer-FIRST, Intelligent Data Analysis Group (Klaus-Robert Muller), and Freie Universitat Berlin, Department of Neurology, Neurophysics Group (Gabriel Curio) Correspondence to Benjamin Blankertz for the BCI II competition (Data set IV). This dataset was recorded from a normal subject during a no-feedback session. The subject sat in a normal chair, relaxed arms resting on the table, fingers in the standard typing position at the computer keyboard. The task was to press with the index and little fingers the corresponding keys in a self-chosen order and timing 'self-paced key typing'. There are two classes: 0 for upcoming left hand movements and 1 for upcoming right hand movements. The experiment consisted of 3 sessions of 6 minutes each. All sessions were conducted on the same day with some minutes break in between. Typing was done at an average speed of 1 key per second. There are 316 train cases and 100 test cases. Each case is a recording of 28 EEG channels of 500 ms length each ending 130 ms before a keypress. This is downsampled at 100 Hz (as recommended) so each channel consists of 50 observations. Channels are in the following order: (F3, F1, Fz, F2, F4, FC5, FC3, FC1, FCz, FC2, FC4, FC6, C5, C3, C1, Cz, C2, C4, C6, CP5, CP3, CP1, CPz, CP2, CP4, CP6, O1, O2). The recording was made using a NeuroScan amplifier and a Ag/AgCl electrode cap from ECI. 28 EEG channels were measured at positions of the international 10/20-system (F, FC, C, and CP rows and O1, O2). Signals were recorded at 1000 Hz with a band-pass filter between 0.05 and 200 Hz. The winning entry achieved an error rate of 16\\% using 3 features from combination of common subspace decomposition and Fisher discriminant, and classification with a perceptron neural network. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d025712",
   "metadata": {},
   "source": [
    "##### Below Code is to Convert arff to csv  (Run only once at the beginning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c4e752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Convert arff to csv\n",
    "# #Run only once\n",
    "\n",
    "# #########################################\n",
    "# # Project   : ARFF to CSV converter     #\n",
    "# # Created   : 10/01/17 11:08:06         #\n",
    "# # Author    : haloboy777                #\n",
    "# # Licence   : MIT                       #\n",
    "# #########################################\n",
    "\n",
    "# # Importing library\n",
    "# import os\n",
    "\n",
    "# # Getting all the arff files from the current directory\n",
    "# files = [arff for arff in os.listdir('.') if arff.endswith(\".arff\")]\n",
    "\n",
    "# # Function for converting arff list to csv list\n",
    "# def toCsv(text):\n",
    "#     data = False\n",
    "#     header = \"\"\n",
    "#     new_content = []\n",
    "#     for line in text:\n",
    "#         if not data:\n",
    "#             if \"@ATTRIBUTE\" in line or \"@attribute\" in line:\n",
    "#                 attributes = line.split()\n",
    "#                 if(\"@attribute\" in line):\n",
    "#                     attri_case = \"@attribute\"\n",
    "#                 else:\n",
    "#                     attri_case = \"@ATTRIBUTE\"\n",
    "#                 column_name = attributes[attributes.index(attri_case) + 1]\n",
    "#                 header = header + column_name + \",\"\n",
    "#             elif \"@DATA\" in line or \"@data\" in line:\n",
    "#                 data = True\n",
    "#                 header = header[:-1]\n",
    "#                 header += '\\n'\n",
    "#                 new_content.append(header)\n",
    "#         else:\n",
    "#             new_content.append(line)\n",
    "#     return new_content\n",
    "\n",
    "\n",
    "# # Main loop for reading and writing files\n",
    "# for file in files:\n",
    "#     with open(file, \"r\") as inFile:\n",
    "#         content = inFile.readlines()\n",
    "#         name, ext = os.path.splitext(inFile.name)\n",
    "#         new = toCsv(content)\n",
    "#         with open(name + \".csv\", \"w\") as outFile:\n",
    "#             outFile.writelines(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4e758d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from time import time\n",
    "warnings.filterwarnings('ignore')\n",
    "from scipy import signal\n",
    "from scipy.signal import butter, lfilter\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pickle\n",
    "import statistics\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from math import log\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367d709e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_factor_list_1 = list(np.linspace(start=0.5, stop=4.5, num=5))\n",
    "scale_factor_list_2 = list(range(1,6))\n",
    "result = [None]*(len(scale_factor_list_1)+len(scale_factor_list_2))\n",
    "result[::2] = scale_factor_list_1\n",
    "result[1::2] = scale_factor_list_2\n",
    "result.append('class')\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc28c1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Butterworth low pass filtering for upscaled time-series\n",
    "def butterworth_lowpass(fc,fs,order):\n",
    "    nyq = 0.5 * fs\n",
    "    Wn = fc/nyq\n",
    "    b,a = butter(order,Wn)\n",
    "    return b,a\n",
    "b, a = butterworth_lowpass(0.49, 1, 4)\n",
    "# print(b,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3c317d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_entropy(baseline_initial):\n",
    "    corr_int_avg = abs(cosine_similarity(baseline_initial.T))\n",
    " \n",
    "    # Calculate eigen-values (w's)\n",
    "    w,v = np.linalg.eig((corr_int_avg))\n",
    "    w = np.real_if_close(w,tol=1)\n",
    "    w_abs = abs(w)\n",
    "    w_sum = np.sum(w_abs)\n",
    "    ent_int = []\n",
    "    for i in w_abs:\n",
    "        if i == 0 or w_sum == 0:\n",
    "            ent_i = 0\n",
    "        else:\n",
    "            if i/w_sum == 0:\n",
    "                ent_i = 0\n",
    "            else:\n",
    "                ent_i = -(i/w_sum)*(log(i/w_sum))\n",
    "                ent_int.append(ent_i)\n",
    "    entropy_initial = np.sum(ent_int)\n",
    "    entropy_initial = (entropy_initial)\n",
    "    return entropy_initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5a5554",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ee_plot(df,ss):\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "    modified_df = df.drop(columns=['groups'])\n",
    "    modified_df_scaled = pd.DataFrame(scaler.fit_transform(modified_df), columns=modified_df.columns)\n",
    "    mws = ss\n",
    "    length = len(modified_df_scaled)\n",
    "    EE = []\n",
    "    while (ss < length):\n",
    "        ee_df = modified_df_scaled.iloc[:ss,:]\n",
    "        entropy = correlation_entropy(ee_df)\n",
    "        EE.append(entropy)\n",
    "        ss += mws\n",
    "    if len(ee_df) < length:\n",
    "        ee_df = modified_df_scaled\n",
    "        entropy = correlation_entropy(ee_df)\n",
    "        EE.append(entropy)\n",
    "    return EE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2d1f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters \n",
    "alpha = 0.5\n",
    "# Input Horizontal DataFrame (final_df_transposed) to the Upscaling Function \n",
    "def upscaling(df,alpha,row_no):\n",
    "    N = df.shape[1]\n",
    "    N_new = int(N/alpha)\n",
    "    y_df = pd.DataFrame(np.zeros((1, N_new)))\n",
    "    p = []\n",
    "    for j in range (1,N+1):\n",
    "        p.append((j/alpha)-1)\n",
    "    for j in range (N_new):\n",
    "        if j == 0:\n",
    "            y_df.iloc[0,j] = df.iloc[row_no,0]\n",
    "        elif j in p:\n",
    "            loc = int((j-1)/(1/alpha))\n",
    "            y_df.iloc[0,j-1] = df.iloc[row_no,(loc)]\n",
    "    return y_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a15cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(path):\n",
    "    df = pd.read_csv(path)\n",
    "    df.columns = range(1, df.shape[1]+1)\n",
    "    df = df.rename(columns={df.shape[1]: 'activity'})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dfb01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User Inputs\n",
    "num_dim = 28\n",
    "test_data_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fbd9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build training dataframe\n",
    "df_tr = pd.DataFrame()\n",
    "for i in range(1,num_dim+1):\n",
    "    path_train = f'/FingerMovements/FingerMovementsDimension{i}_TRAIN.csv'\n",
    "    df =  read(path_train)\n",
    "    df = df.assign(Dim=i)\n",
    "    df_tr = df_tr.append(df)\n",
    "display(df_tr.head())\n",
    "sample_size = int(((df_tr.shape[1])-2)/50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe98d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build testing dataframe\n",
    "df_te = pd.DataFrame()\n",
    "for i in range(1,num_dim+1):\n",
    "    path_test = f'/FingerMovements/FingerMovementsDimension{i}_TEST.csv'\n",
    "    df =  read(path_test)\n",
    "    df = df.assign(Dim=i)\n",
    "    df_te = df_te.append(df)\n",
    "display(df_te.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec3acfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr['Sub'] = df_tr.groupby(['activity','Dim']).cumcount() + 1\n",
    "df_te['Sub'] = df_te.groupby(['activity','Dim']).cumcount() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e27c1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form a class list of unique activities\n",
    "class_list = list(df_tr.activity.unique())\n",
    "class_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b68b8c7",
   "metadata": {},
   "source": [
    "# EE_Trend_All_Values and slopes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13577dee",
   "metadata": {},
   "source": [
    "### Build train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587cf3a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EE_values_train = pd.DataFrame(columns = result)\n",
    "cols = [i for i in range(1,1001)]\n",
    "EE_values_train_mod = pd.DataFrame(columns = cols)\n",
    "EE_values_train_slope = pd.DataFrame(columns = cols)\n",
    " \n",
    "for c in class_list:\n",
    "    df_tr_act = df_tr[df_tr['activity']== c]\n",
    "    number = df_tr_act.Sub.nunique()\n",
    "    \n",
    "    for i in range(1,number+1):\n",
    "        df_tr_new = df_tr_act[(df_tr_act['Sub']== i)]\n",
    "\n",
    "        df_tr_new = (df_tr_new.drop(columns=['activity','Sub','Dim'])).T\n",
    "        df_tr_new = df_tr_new.reset_index().drop(columns='index')\n",
    "        df_tr_new.columns = range(1,num_dim+1)\n",
    "        df_tr_new = df_tr_new.astype(float)\n",
    "        \n",
    "        df_tr_new_transposed = df_tr_new.T\n",
    "        n_cols = int((df_tr_new_transposed.shape[1]/alpha))\n",
    "        df_upfil = pd.DataFrame(columns = range(n_cols))\n",
    "        for j in range(df_tr_new_transposed.shape[0]):\n",
    "            upscaled = upscaling(df_tr_new_transposed,alpha,j)\n",
    "            filtered_row = lfilter(b, a,upscaled)\n",
    "            df_upfil = df_upfil.append(pd.DataFrame(filtered_row))\n",
    "        df_upfil_trans = df_upfil.T\n",
    "        df_upfil_trans.columns = range(1,num_dim+1)\n",
    "            \n",
    "        df_upfil = df_upfil_trans.reset_index()\n",
    "        df_tr_new = df_tr_new.reset_index()\n",
    "        EE_scaled_basic = []\n",
    "        EE_scaled = []\n",
    "        EE_scaled_slope = []\n",
    "        for scale_factor in result[:-1]:\n",
    "            if type(scale_factor) == int:\n",
    "                df_tr_new['groups'] = df_tr_new['index'].apply(lambda x: round(x/(scale_factor)-0.36,0)).astype(int) \n",
    "                df_tr_new_1 = df_tr_new.groupby('groups').mean().reset_index().drop(columns=['index'])\n",
    "\n",
    "                EE_1 = ee_plot(df_tr_new_1,sample_size)\n",
    "                # Calculate trend\n",
    "                EE_scaled_basic.append((EE_1[-1]-EE_1[0]) / len(EE_1))\n",
    "                \n",
    "                # Include all Eigen values\n",
    "                EE_scaled.extend(EE_1)\n",
    "                \n",
    "                # Calculate slopes\n",
    "                slopes = []\n",
    "                slope_len = len(EE_1)\n",
    "                for a in range(1,slope_len):\n",
    "                    slope = (EE_1[a]-EE_1[0])/(a+1)\n",
    "                    slopes.append(slope)\n",
    "                EE_scaled_slope.extend(slopes)\n",
    "                \n",
    "                print(f'class {c}; subject {i}; scale_factor {scale_factor}; slop_len{slope_len}')\n",
    "                    \n",
    "            else:\n",
    "                q = int(scale_factor/alpha)\n",
    "                df_upfil = df_upfil.apply(pd.to_numeric)\n",
    "\n",
    "                df_upfil['groups'] = df_upfil['index'].apply(lambda x: round(x/(q)-0.36,0)).astype(int) \n",
    "                df_upfil_1 = df_upfil.groupby('groups').mean().reset_index().drop(columns=['index'])\n",
    "    \n",
    "                EE_1 = ee_plot(df_upfil_1,sample_size)\n",
    "    \n",
    "                # Calculate trend\n",
    "                EE_scaled_basic.append((EE_1[-1]-EE_1[0]) / len(EE_1))\n",
    "                \n",
    "                # Include all Eigen values\n",
    "                EE_scaled.extend(EE_1)\n",
    "                \n",
    "                # Calculate slopes\n",
    "                slopes = []\n",
    "                slope_len = len(EE_1)\n",
    "                for a in range(1,slope_len):\n",
    "                    slope = (EE_1[a]-EE_1[0])/(a+1)\n",
    "                    slopes.append(slope)\n",
    "                EE_scaled_slope.extend(slopes)\n",
    "                \n",
    "                print(f'class {c}; subject {i}; scale_factor {scale_factor}; slop_len{slope_len}')\n",
    "                \n",
    "        EE_scaled_basic.append(c)\n",
    "        EE_values_train = EE_values_train.append(pd.DataFrame([EE_scaled_basic], columns=result), ignore_index=True)\n",
    "    \n",
    "        EE_scaled.append(c)\n",
    "        nan_cols = [np.nan for k in range(1,1001-len(EE_scaled))]\n",
    "        EE_scaled.extend(nan_cols)\n",
    "        EE_values_train_mod = EE_values_train_mod.append(pd.DataFrame([EE_scaled], columns=cols), ignore_index=True)\n",
    "        \n",
    "        EE_scaled_slope.append(c)\n",
    "        nan_cols = [np.nan for k in range(1,1001-len(EE_scaled_slope))]\n",
    "        EE_scaled_slope.extend(nan_cols)\n",
    "        EE_values_train_slope = EE_values_train_slope.append(pd.DataFrame([EE_scaled_slope], columns=cols), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522c9ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "EE_values_train_mod = EE_values_train_mod.dropna(axis=1)\n",
    "EE_values_train_mod.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe1702b",
   "metadata": {},
   "outputs": [],
   "source": [
    "EE_values_train_slope = EE_values_train_slope.dropna(axis=1)\n",
    "EE_values_train_slope.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb91398b",
   "metadata": {},
   "source": [
    "### Build test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ced329",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EE_values_test = pd.DataFrame(columns = result)\n",
    "cols = [i for i in range(1,1001)]\n",
    "EE_values_test_mod = pd.DataFrame(columns = cols)\n",
    "EE_values_test_slope = pd.DataFrame(columns = cols)\n",
    "\n",
    "for c in class_list:\n",
    "    df_te_act = df_te[df_te['activity']== c]\n",
    "    number = df_te_act.Sub.nunique()\n",
    "    \n",
    "    for i in range(1,number+1):\n",
    "        df_te_new = df_te_act[(df_te_act['Sub']== i)]\n",
    "        df_te_new = (df_te_new.drop(columns=['activity','Sub','Dim'])).T\n",
    "        df_te_new = df_te_new.reset_index().drop(columns='index')\n",
    "        df_te_new.columns = range(1,num_dim+1)\n",
    "        df_te_new = df_te_new.astype(float)\n",
    "        \n",
    "        df_te_new_transposed = df_te_new.T\n",
    "        n_cols = int((df_te_new_transposed.shape[1]/alpha))\n",
    "        \n",
    "        df_upfil = pd.DataFrame(columns = range(n_cols))\n",
    "        for j in range(df_te_new_transposed.shape[0]):\n",
    "            upscaled = upscaling(df_te_new_transposed,alpha,j)\n",
    "            filtered_row = lfilter(b, a,upscaled)\n",
    "            df_upfil = df_upfil.append(pd.DataFrame(filtered_row))\n",
    "        df_upfil_trans = df_upfil.T\n",
    "        df_upfil_trans.columns = range(1,num_dim+1)\n",
    "            \n",
    "        df_upfil = df_upfil_trans.reset_index()\n",
    "        df_te_new = df_te_new.reset_index()\n",
    "\n",
    "        EE_scaled_basic = []\n",
    "        EE_scaled = []\n",
    "        EE_scaled_slope = []\n",
    "        \n",
    "        for scale_factor in result[:-1]:\n",
    "            if type(scale_factor) == int:\n",
    "                df_te_new['groups'] = df_te_new['index'].apply(lambda x: round(x/(scale_factor)-0.36,0)).astype(int) \n",
    "                df_te_new_1 = df_te_new.groupby('groups').mean().reset_index().drop(columns=['index'])\n",
    "                \n",
    "                EE_1 = ee_plot(df_te_new_1,sample_size)\n",
    "                # Calculate trend\n",
    "                EE_scaled_basic.append((EE_1[-1]-EE_1[0]) / len(EE_1))\n",
    "                \n",
    "                # Include all Eigen values\n",
    "                EE_scaled.extend(EE_1)\n",
    "                \n",
    "                # Calculate slopes\n",
    "                slopes = []\n",
    "                slope_len = len(EE_1)\n",
    "                for a in range(1,slope_len):\n",
    "                    slope = (EE_1[a]-EE_1[0])/(a+1)\n",
    "                    slopes.append(slope)\n",
    "                EE_scaled_slope.extend(slopes)\n",
    "                \n",
    "                print(f'class {c}; subject {i}; scale_factor {scale_factor}; slope_len{slope_len}')\n",
    "                \n",
    "\n",
    "            else:\n",
    "                q = int(scale_factor/alpha)\n",
    "                df_upfil = df_upfil.apply(pd.to_numeric)\n",
    "\n",
    "                df_upfil['groups'] = df_upfil['index'].apply(lambda x: round(x/(q)-0.36,0)).astype(int) \n",
    "                df_upfil_1 = df_upfil.groupby('groups').mean().reset_index().drop(columns=['index'])\n",
    "\n",
    "                EE_1 = ee_plot(df_upfil_1,sample_size)\n",
    "    \n",
    "                # Calculate trend\n",
    "                EE_scaled_basic.append((EE_1[-1]-EE_1[0]) / len(EE_1))\n",
    "                \n",
    "                # Include all Eigen values\n",
    "                EE_scaled.extend(EE_1)\n",
    "                \n",
    "                # Calculate slopes\n",
    "                slopes = []\n",
    "                slope_len = len(EE_1)\n",
    "                for a in range(1,slope_len):\n",
    "                    slope = (EE_1[a]-EE_1[0])/(a+1)\n",
    "                    slopes.append(slope)\n",
    "                EE_scaled_slope.extend(slopes)\n",
    "                \n",
    "                print(f'class {c}; subject {i}; scale_factor {scale_factor}; slope_len{slope_len}')\n",
    "                \n",
    "        EE_scaled_basic.append(c)\n",
    "        EE_values_test = EE_values_test.append(pd.DataFrame([EE_scaled_basic], columns=result), ignore_index=True)\n",
    "    \n",
    "        EE_scaled.append(c)\n",
    "        nan_cols = [np.nan for k in range(1,1001-len(EE_scaled))]\n",
    "        EE_scaled.extend(nan_cols)\n",
    "        EE_values_test_mod = EE_values_test_mod.append(pd.DataFrame([EE_scaled], columns=cols), ignore_index=True)\n",
    "        \n",
    "        EE_scaled_slope.append(c)\n",
    "        nan_cols = [np.nan for k in range(1,1001-len(EE_scaled_slope))]\n",
    "        EE_scaled_slope.extend(nan_cols)\n",
    "        EE_values_test_slope = EE_values_test_slope.append(pd.DataFrame([EE_scaled_slope], columns=cols), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8c8678",
   "metadata": {},
   "outputs": [],
   "source": [
    "EE_values_train_slope.to_csv('EE_values_train_slope.csv')\n",
    "EE_values_test_slope.to_csv('EE_values_test_slope.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
